{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T06:53:49.197655",
     "start_time": "2017-06-20T15:53:46.785329+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "use_cuda = True\n",
    "device_id = 3\n",
    "from tensorflow.contrib.keras.python.keras.datasets.imdb import load_data, get_word_index\n",
    "\n",
    "max_features = 5000\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T06:53:50.613005",
     "start_time": "2017-06-20T15:53:50.599692+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T06:54:36.427758",
     "start_time": "2017-06-20T15:53:50.816215+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data(num_words=max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T06:54:36.434152",
     "start_time": "2017-06-20T15:54:36.429401+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad(tensor, length):\n",
    "    return torch.cat([tensor, tensor.new(length - tensor.size(0),*tensor.size()[1:]).zero_()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T06:54:36.464192",
     "start_time": "2017-06-20T15:54:36.435198+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sortedText(idx, xs, ys):\n",
    "    batch_xs = xs[idx]\n",
    "    batch_ys = ys[idx]\n",
    "    lengths = np.array([len(x) for x in batch_xs])\n",
    "    sort_idx = np.argsort(lengths)[::-1]\n",
    "    return batch_xs[sort_idx], lengths[sort_idx], batch_ys[sort_idx]\n",
    "\n",
    "\n",
    "def textTensor(idx, xs, ys):\n",
    "    batch_xs, lengths, batch_ys = sortedText(idx, xs, ys)\n",
    "    max_length = lengths[0]\n",
    "    return torch.cat([pad(torch.Tensor(x), max_length).view(max_length, 1)\n",
    "                      for x in batch_xs], 1).long(), list(lengths), torch.FloatTensor(batch_ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T08:19:47.976077",
     "start_time": "2017-06-20T17:19:47.945661+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IMDB_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, n_layers=1):\n",
    "        super(IMDB_Classifier, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size/2, bidirectional=True)\n",
    "        self.dense = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, word_input, input_length, hidden=None):\n",
    "        output = self.embedding(word_input)\n",
    "        output = pack_padded_sequence(output, input_length)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = torch.cat([hidden[0][0], hidden[0][1]], 1)\n",
    "        output = self.dropout(output)\n",
    "        output = F.sigmoid(self.dense(output))\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def initHidden(self):\n",
    "        hidden = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "        cell = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "        return (hidden.cuda(device_id), cell.cuda(device_id)) if use_cuda else (hidden, cell) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T08:19:48.744277",
     "start_time": "2017-06-20T17:19:48.728303+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_last_step_indices(lengths):\n",
    "    n_lengths = len(lengths)\n",
    "    rev_lengths = lengths[::-1]\n",
    "    rev_lengths_sum = torch.LongTensor(rev_lengths).cumsum(0)\n",
    "    return torch.LongTensor([(n_lengths - i - 1) * length + rev_lengths_sum[i] - 1\n",
    "                         for i, length in enumerate(rev_lengths)][::-1])\n",
    "\n",
    "\n",
    "def get_last_step_tensor(packed_sequence, lengths):\n",
    "    indices = Variable(torch.LongTensor(get_last_step_indices(lengths)))\n",
    "    if packed_sequence.data.data.is_cuda:\n",
    "        indices = indices.cuda(packed_sequence.data.data.get_device())\n",
    "    last_step = packed_sequence.data.index_select(0, indices)\n",
    "    return last_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T08:19:49.275757",
     "start_time": "2017-06-20T17:19:49.264529+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    idx, y = iter(train_loader).next()\n",
    "    x, lengths, y = textTensor(idx, x_train, y_train)\n",
    "    x, y = Variable(x), Variable(y)\n",
    "    print('input_batches', x.size()) \n",
    "    model = IMDB_Classifier(max_features, 2, 32)\n",
    "    model = model.cuda() if use_cuda else model\n",
    "    (x, y) = (x.cuda(), y.cuda()) if use_cuda else (x, y)\n",
    "    output = model(x, lengths)\n",
    "    print('output_batches', output.size()) \n",
    "    cretrion = nn.BCELoss()\n",
    "\n",
    "    print(cretrion(output, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-20T08:24:01.184081",
     "start_time": "2017-06-20T17:19:50.055605+09:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 10, batch_loss: 0.695220530033\n",
      "batch: 20, batch_loss: 0.692368993163\n",
      "batch: 30, batch_loss: 0.690876714389\n",
      "batch: 40, batch_loss: 0.690659186244\n",
      "batch: 50, batch_loss: 0.689180973768\n",
      "batch: 60, batch_loss: 0.6895878613\n",
      "batch: 70, batch_loss: 0.689470046759\n",
      "batch: 80, batch_loss: 0.688695345074\n",
      "batch: 90, batch_loss: 0.688032517831\n",
      "batch: 100, batch_loss: 0.687166379094\n",
      "batch: 110, batch_loss: 0.686698469791\n",
      "batch: 120, batch_loss: 0.685625120004\n",
      "batch: 130, batch_loss: 0.684387684785\n",
      "batch: 140, batch_loss: 0.683821835262\n",
      "batch: 150, batch_loss: 0.683342698018\n",
      "batch: 160, batch_loss: 0.682021187246\n",
      "batch: 170, batch_loss: 0.68045007902\n",
      "batch: 180, batch_loss: 0.679582681921\n",
      "batch: 190, batch_loss: 0.679153075657\n",
      "batch: 200, batch_loss: 0.677671817839\n",
      "batch: 210, batch_loss: 0.676283865316\n",
      "batch: 220, batch_loss: 0.675144485994\n",
      "batch: 230, batch_loss: 0.674321341774\n",
      "batch: 240, batch_loss: 0.671996882061\n",
      "batch: 250, batch_loss: 0.669673966169\n",
      "batch: 260, batch_loss: 0.66771438523\n",
      "batch: 270, batch_loss: 0.665007077323\n",
      "batch: 280, batch_loss: 0.662693741918\n",
      "batch: 290, batch_loss: 0.660914065715\n",
      "batch: 300, batch_loss: 0.65964061141\n",
      "batch: 310, batch_loss: 0.657358268961\n",
      "batch: 320, batch_loss: 0.656505993009\n",
      "batch: 330, batch_loss: 0.652993498878\n",
      "batch: 340, batch_loss: 0.651532071566\n",
      "batch: 350, batch_loss: 0.649175160016\n",
      "batch: 360, batch_loss: 0.646767697401\n",
      "batch: 370, batch_loss: 0.643483591482\n",
      "batch: 380, batch_loss: 0.640689237651\n",
      "batch: 390, batch_loss: 0.637902062444\n",
      "batch: 400, batch_loss: 0.63562375389\n",
      "batch: 410, batch_loss: 0.633491977468\n",
      "batch: 420, batch_loss: 0.631716295793\n",
      "batch: 430, batch_loss: 0.631951796731\n",
      "batch: 440, batch_loss: 0.629918905415\n",
      "batch: 450, batch_loss: 0.627780352036\n",
      "batch: 460, batch_loss: 0.626036404915\n",
      "batch: 470, batch_loss: 0.62461346521\n",
      "batch: 480, batch_loss: 0.622248856537\n",
      "batch: 490, batch_loss: 0.619792926251\n",
      "batch: 500, batch_loss: 0.617267856896\n",
      "batch: 510, batch_loss: 0.614524357927\n",
      "batch: 520, batch_loss: 0.611952369775\n",
      "batch: 530, batch_loss: 0.609881225678\n",
      "batch: 540, batch_loss: 0.611038613319\n",
      "batch: 550, batch_loss: 0.611416154558\n",
      "batch: 560, batch_loss: 0.61051775861\n",
      "batch: 570, batch_loss: 0.609358081274\n",
      "batch: 580, batch_loss: 0.607736853382\n",
      "batch: 590, batch_loss: 0.606040419562\n",
      "batch: 600, batch_loss: 0.604871755044\n",
      "batch: 610, batch_loss: 0.602806901297\n",
      "batch: 620, batch_loss: 0.601460733866\n",
      "batch: 630, batch_loss: 0.599959771974\n",
      "batch: 640, batch_loss: 0.597650434962\n",
      "batch: 650, batch_loss: 0.596739330567\n",
      "batch: 660, batch_loss: 0.595081950724\n",
      "batch: 670, batch_loss: 0.593912194336\n",
      "batch: 680, batch_loss: 0.592799637861\n",
      "batch: 690, batch_loss: 0.592092969841\n",
      "batch: 700, batch_loss: 0.590618904233\n",
      "batch: 710, batch_loss: 0.588679827687\n",
      "batch: 720, batch_loss: 0.58678827708\n",
      "batch: 730, batch_loss: 0.585124188907\n",
      "batch: 740, batch_loss: 0.583259299478\n",
      "batch: 750, batch_loss: 0.582049331586\n",
      "batch: 760, batch_loss: 0.580407416938\n",
      "batch: 770, batch_loss: 0.578638231174\n",
      "batch: 780, batch_loss: 0.57734301969\n",
      "Epoch: 1, time: 3m 44s (- 52m 17s), loss: 0.577216731754\n",
      "Accuracy of the network on the 25000 texts: 81.108 %\n",
      "batch: 10, batch_loss: 0.362792870402\n",
      "batch: 20, batch_loss: 0.376517445594\n",
      "batch: 30, batch_loss: 0.375578880807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-25039c7fed15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/pyenv/versions/latest_mod/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = IMDB_Classifier(max_features, 128, batch_size)\n",
    "clf = clf.cuda(device_id) if use_cuda else clf\n",
    "optimizer = optim.Adam(clf.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, 101):\n",
    "    losses = 0\n",
    "    indices = np.random.permutation(np.array(range(25000)))\n",
    "    for i in range(1, indices.shape[0] / 32 + 1):\n",
    "        x, lengths, y = textTensor(indices[(i-1)*32:i*32], x_train, y_train)\n",
    "        x, y = Variable(x), Variable(y)\n",
    "\n",
    "        (x, y) = (x.cuda(device_id), y.cuda(device_id)) if use_cuda else (x, y)\n",
    "\n",
    "        output = clf(x, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        losses += loss.data[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"batch: {}, batch_loss: {}\".format(i, losses / i))\n",
    "\n",
    "    print(\"Epoch: {}, time: {}, loss: {}\".format(epoch, timeSince(start, float(epoch) / epochs), losses/(i)))\n",
    "\n",
    "\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(250):\n",
    "        x, lengths, y = textTensor(range(25000)[i*100:(i+1)*100], x_train, y_train)\n",
    "        #x, lengths, y = textTensor(torch.LongTensor(range(32)), x_train, y_train)\n",
    "        x, y = Variable(x, volatile=True), Variable(y)\n",
    "        (x, y) = (x.cuda(device_id), y.cuda(device_id)) if use_cuda else (x, y)\n",
    "        output = clf(x, lengths)\n",
    "        output = output > 0.5\n",
    "        correct += (output.float() == y).sum().data[0]\n",
    "        total += y.size(0)\n",
    "    print('Accuracy of the network on the {} texts: {} %'.format(y_test.shape[0], 100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LATEST",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
